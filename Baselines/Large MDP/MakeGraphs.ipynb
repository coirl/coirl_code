{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs of \"Large MDP\" Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def load_obj(name,dirname):\n",
    "    with open(dirname + 'obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 18})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graphs():\n",
    "    repeats = 5\n",
    "    iters = 200\n",
    "    dlistB = []\n",
    "    for i in range(repeats):\n",
    "        try:\n",
    "            dlistB.append(load_obj(\"al_values\"+str(i),'Grid world/'))\n",
    "        except:\n",
    "            pass\n",
    "    dlistC = []\n",
    "    for i in range(repeats):\n",
    "        try:\n",
    "            dlistC.append(load_obj(\"psgd_values\"+str(i),'Grid world/'))\n",
    "        except:\n",
    "            pass\n",
    "   \n",
    "\n",
    "    TBSTS = True\n",
    "    test_expert_values = []\n",
    "    test_agent_runtimes = []\n",
    "    test_agent_values = []\n",
    "    test_agent_accurs = []\n",
    "    cont_vec = [5,10,15,20,30,40]\n",
    "    for trainset in range(repeats):\n",
    "        test_expert_values.append([])\n",
    "        test_agent_values.append([])\n",
    "        test_agent_runtimes.append([])\n",
    "        test_agent_accurs.append([])\n",
    "        for d in dlistB:\n",
    "            num_iterz = d[\"iteration_nums\"]\n",
    "            TBSTS = True\n",
    "            for contexts in cont_vec:\n",
    "                test_expert_values[-1].append(d[trainset,\"test_value_expert\",contexts])\n",
    "                test_agent_runtimes[-1].append(d[trainset,\"runtime\",contexts])\n",
    "                test_agent_accurs[-1].append(d[trainset,\"accuracy\",contexts])\n",
    "                test_agent_values[-1].append(d[trainset,\"test_value\",contexts])\n",
    "\n",
    "            runtimes_mat = np.zeros([len(cont_vec)*len(num_iterz),repeats])\n",
    "            accurs_mat = np.zeros([len(cont_vec)*len(num_iterz),repeats])\n",
    "            values_mat = np.zeros([len(cont_vec)*len(num_iterz),repeats])\n",
    "            \n",
    "    for cont_i in range(len(cont_vec)):\n",
    "        for trainset in range(repeats):\n",
    "                \n",
    "            runtimes_mat[cont_i*len(num_iterz):cont_i*len(num_iterz)+len(num_iterz),trainset] = test_agent_runtimes[trainset][cont_i]\n",
    "            accurs_mat[cont_i*len(num_iterz):cont_i*len(num_iterz)+len(num_iterz),trainset] = test_agent_accurs[trainset][cont_i]\n",
    "            for iit in range(len(num_iterz)):\n",
    "                values_mat[cont_i*len(num_iterz) + iit,trainset] = test_agent_values[trainset][cont_i][iit]/test_expert_values[trainset][cont_i]\n",
    "            \n",
    "    runtimes_mean = runtimes_mat.mean(axis=1)\n",
    "    accurs_mean = accurs_mat.mean(axis=1)\n",
    "    values_mean = (100*values_mat).mean(axis=1)\n",
    "    runtimes_std = runtimes_mat.std(axis=1)\n",
    "    accurs_std = accurs_mat.std(axis=1)\n",
    "    values_std = (100*values_mat).std(axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    TCSTS = True\n",
    "    test_expert_valuesC = []\n",
    "    test_agent_runtimesC = []\n",
    "    test_agent_valuesC = []\n",
    "    test_agent_accursC = []\n",
    "    cont_vec = [5,10,15,20,30,40]\n",
    "    for trainset in range(repeats):\n",
    "        test_expert_values.append([])\n",
    "        test_agent_valuesC.append([])\n",
    "        test_agent_runtimesC.append([])\n",
    "        test_agent_accursC.append([])\n",
    "        for d in dlistC:\n",
    "            num_iterzC = d[\"iteration_nums\"]\n",
    "            TCSTS = True\n",
    "            for contexts in cont_vec:\n",
    "                test_agent_runtimesC[-1].append(d[trainset,\"runtime\",contexts])\n",
    "                test_agent_accursC[-1].append(d[trainset,\"accuracy\",contexts])\n",
    "                test_agent_valuesC[-1].append(d[trainset,\"test_value\",contexts])\n",
    "\n",
    "            runtimes_matC = np.zeros([len(cont_vec)*len(num_iterzC),repeats])\n",
    "            accurs_matC = np.zeros([len(cont_vec)*len(num_iterzC),repeats])\n",
    "            values_matC = np.zeros([len(cont_vec)*len(num_iterzC),repeats])\n",
    "            \n",
    "    for cont_i in range(len(cont_vec)):\n",
    "        for trainset in range(repeats):\n",
    "            runtimes_matC[cont_i*len(num_iterzC):cont_i*len(num_iterzC)+len(num_iterzC),trainset] = test_agent_runtimesC[trainset][cont_i]\n",
    "            accurs_matC[cont_i*len(num_iterzC):cont_i*len(num_iterzC)+len(num_iterzC),trainset] = test_agent_accursC[trainset][cont_i]\n",
    "            for iit in range(len(num_iterzC)): \n",
    "                values_matC[cont_i*len(num_iterzC)+iit,trainset] = test_agent_valuesC[trainset][cont_i][iit]/test_expert_values[trainset][cont_i]\n",
    "            \n",
    "    runtimes_meanC = runtimes_matC.mean(axis=1)\n",
    "    accurs_meanC = accurs_matC.mean(axis=1)\n",
    "    values_meanC = (100*values_matC).mean(axis=1)\n",
    "    runtimes_stdC = runtimes_matC.std(axis=1)\n",
    "    accurs_stdC = accurs_matC.std(axis=1)\n",
    "    values_stdC = (100*values_matC).std(axis=1)\n",
    "\n",
    "    final_runtimes_mean = np.zeros(len(cont_vec))\n",
    "    final_runtimes_meanC = np.zeros(len(cont_vec))\n",
    "    final_runtimes_std = np.zeros(len(cont_vec))\n",
    "    final_runtimes_stdC = np.zeros(len(cont_vec))\n",
    "    \n",
    "    final_iter_vec = np.ones(len(cont_vec))*(num_iterz[-1]-1) \n",
    "    final_iter_vecC = np.ones(len(cont_vec))*(num_iterzC[-1]-1)\n",
    "    for cont_i in range(len(cont_vec)):\n",
    "        final_runtimes_mean[cont_i] = runtimes_mean[cont_i*len(num_iterz) + int(final_iter_vec[cont_i])]\n",
    "        final_runtimes_meanC[cont_i] = runtimes_meanC[cont_i*len(num_iterzC) + int(final_iter_vecC[cont_i])]\n",
    "        final_runtimes_std[cont_i] = runtimes_std[cont_i*len(num_iterz) + int(final_iter_vec[cont_i])]\n",
    "        final_runtimes_stdC[cont_i] = runtimes_stdC[cont_i*len(num_iterzC) + int(final_iter_vecC[cont_i])]\n",
    "    \n",
    "    indices = cont_vec\n",
    "    fig1 = plt.figure(figsize=(6,4.5))\n",
    "    p1 = fig1.add_subplot(111)\n",
    "    p1.errorbar(indices,final_runtimes_mean,yerr=final_runtimes_std, fmt='o', label=\"AL (Abbeel & NG)\", color='r',capsize=4)\n",
    "    p1.errorbar(indices,final_runtimes_meanC,yerr=final_runtimes_stdC, fmt='o', label=\"COIRL (PSGD)\", color='b',capsize=4)\n",
    "    p1.set_xlabel(\"Contexts\")\n",
    "    p1.set_ylabel(\"Run time [sec]\")\n",
    "    p1.tick_params('y', colors='k')\n",
    "    p1.legend(loc=2)    \n",
    "    p1.yaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "    plt.show()\n",
    "    \n",
    "    fig2 = plt.figure(figsize=(18,11))\n",
    "    fig3 = plt.figure(figsize=(18,11))\n",
    "    p2 = []\n",
    "    p3 = []\n",
    "    big1 = fig2.add_subplot(111)\n",
    "    big2 = fig3.add_subplot(111)\n",
    "    big1.spines['top'].set_color('none')\n",
    "    big1.spines['bottom'].set_color('none')\n",
    "    big1.spines['left'].set_color('none')\n",
    "    big1.spines['right'].set_color('none')\n",
    "    big1.tick_params(labelcolor='w', top=False, bottom=False, left=False, right=False)\n",
    "    big2.spines['top'].set_color('none')\n",
    "    big2.spines['bottom'].set_color('none')\n",
    "    big2.spines['left'].set_color('none')\n",
    "    big2.spines['right'].set_color('none')\n",
    "    big2.tick_params(labelcolor='w', top=False, bottom=False, left=False, right=False)\n",
    "    start_vals = [20,20,22,24,25,25]\n",
    "    start_accurs = [12,12,13,14,14,14]\n",
    "    loccx = 1\n",
    "    loccy = 1\n",
    "    locc = 0\n",
    "    \n",
    "    for cont_i in range(len(cont_vec)):\n",
    "        loccy += 1\n",
    "        locc += 1\n",
    "        if loccy > 2:\n",
    "            loccy = 0\n",
    "            loccx += 1\n",
    "        p2 = fig2.add_subplot(230 + locc)\n",
    "        i_start = cont_i*len(num_iterz)\n",
    "        i_end = (cont_i+1)*len(num_iterz)\n",
    "        i_startC = cont_i*len(num_iterzC)\n",
    "        i_endC = (cont_i+1)*len(num_iterzC)\n",
    "        \n",
    "        p2.plot(np.insert(runtimes_mean[i_start:i_end],0,0),np.insert(accurs_mean[i_start:i_end],0,start_accurs[cont_i]), label=\"AL (Abbeel & NG)\", color='r')\n",
    "        p2.fill_between(runtimes_mean[i_start:i_end],accurs_mean[i_start:i_end] - accurs_std[i_start:i_end], accurs_mean[i_start:i_end] + accurs_std[i_start:i_end], color='r', alpha=0.1)\n",
    "        p2.plot(np.insert(runtimes_meanC[i_startC:i_endC],0,0),np.insert(accurs_meanC[i_startC:i_endC],0,start_accurs[cont_i]), label=\"COIRL (PSGD)\", color='b')\n",
    "        p2.fill_between(runtimes_meanC[i_startC:i_endC],accurs_meanC[i_startC:i_endC] - accurs_stdC[i_startC:i_endC], accurs_meanC[i_startC:i_endC] + accurs_stdC[i_startC:i_endC], color='b', alpha=0.1)\n",
    "        p2.set_title(str(cont_vec[cont_i]) + \" Contexts\")\n",
    "        big1.set_xlabel(\"Run time [sec]\")\n",
    "        big1.set_ylabel(\"% Hit\")\n",
    "        p2.set_ylim([40,100])\n",
    "\n",
    "        handles, labels = p2.get_legend_handles_labels()\n",
    "        fig2.legend(handles, labels, loc='upper center')\n",
    "        p3 = fig3.add_subplot(230 + locc)\n",
    "        p3.plot(np.insert(runtimes_mean[i_start:i_end],0,0),np.insert(values_mean[i_start:i_end],0,start_vals[cont_i]), label=\"AL (Abbeel & NG)\", color='r')\n",
    "        p3.fill_between(runtimes_mean[i_start:i_end],values_mean[i_start:i_end] - values_std[i_start:i_end], values_mean[i_start:i_end] + values_std[i_start:i_end], color='r', alpha=0.1)\n",
    "        p3.plot(np.insert(runtimes_meanC[i_startC:i_endC],0,0),np.insert(values_meanC[i_startC:i_endC],0,start_vals[cont_i]), label=\"COIRL (PSGD)\", color='b')\n",
    "        p3.fill_between(runtimes_meanC[i_startC:i_endC],values_meanC[i_startC:i_endC] - values_stdC[i_startC:i_endC], values_meanC[i_startC:i_endC] + values_stdC[i_startC:i_endC], color='b', alpha=0.1)\n",
    "        big2.set_xlabel(\"Run time [sec]\")\n",
    "        big2.set_ylabel(\"% Value of Expert\")\n",
    "        p3.set_title(str(cont_vec[cont_i]) + \" Contexts\")\n",
    "        p3.set_ylim([80,100])\n",
    "\n",
    "        handles, labels = p3.get_legend_handles_labels()\n",
    "        fig3.legend(handles, labels, loc='upper center')\n",
    "\n",
    "plt.show()    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=make_graphs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
